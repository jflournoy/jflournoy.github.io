[
  {
    "objectID": "consulting.html",
    "href": "consulting.html",
    "title": "Consulting Services",
    "section": "",
    "text": "And if you don’t have the data yet, I’ll help you gather it. Whether you’re navigating uncertainty, testing a new direction, or refining what already works, I help turn raw data into strategic advantage. My work establishes insights that enables progress you can measure.\n\n\n\nI partner with teams to design and execute rigorous, reproducible research—combining advanced statistical methods, applied data science, and robust study design to inform decision-making.\n\n\n\n\nOr email me directly at consulting@johnflournoy.science\n\n\n \n\nName \n\n\nEmail \n\n\nMessage\n\n\n\n\n\nSend Message\n\n\n\n\n\n\n\nCan you help us decide what questions are worth answering?\nYes. I’ll work with you to clarify your goals and refine your questions—so we focus on what matters and design the right strategy to answer it.\nWe don’t have clean data—can you still help us?\nAbsolutely. Data cleaning is half the work, and I consider it a core competency. I can work with messy, incomplete, or scattered data and help you turn it into something useful.\nWill this work lead to tangible improvements we can present to leadership?\nThat’s the goal. From the start, we’ll align on questions that map directly to decisions, priorities, and key indicators. What goes into the slide deck will also move things forward.\nJust need a plan or a second set of eyes?\nPackages start at $500 for things like analysis or research design review, or broad research planning advice. If you need something more involved, we can scope that together.\n\n\n\nI earned my PhD in Psychology at the University of Oregon, completed post-doctoral work at Harvard where I was hired on as a Research Associate, and most recently led technologist-focused science as Principal Research Scientist at the Developer Success Lab. I’m available to bring that same rigor directly to your projects.\n\nStatistical Methods & Modeling\n\nHierarchical Bayesian models, multilevel & SEM frameworks\nSimulation studies, causal inference, power analyses\nReproducible pipelines in R, Stan, Python, SQL\n\nResearch & Study Design\n\nExperimental, observational, quasi-experimental, and mixed-methods approaches\nPre-registration, sampling frameworks, measurement validation\nIntegrated qualitative insights\nSurvey-embedded experiments\n\nSurveys & Instrument Development\n\nQuestion wording, scale construction, psychometric evaluation\nSampling strategies and weighting for representativeness\nMissing-data handling, data cleaning\n\nApplied Qualitative Integration\n\nQualitative interviewing\nThematic transcript coding\nQuantitative natural language processing\n\n\n\n\n\nAll work is package-based—we collaborate on a plan that includes a block of hours each month for live collaboration (video calls, async chat) and all deliverables (analysis scripts, study plans, slide decks). Packages scale from a quick sanity check to an accelerated, end-to-end research sprint.\n\n\n\n\nBroad research experience: 15+ years across multiple methods and domains of inquiry\nTransparent workflows: Code and analyses shared via version control with clear documentation and we work together to make sure everything is reproducible.\nCrafted for your audience: Reports, slide decks, and visuals are created to be understood by stakeholders with a variety of expertise.\nCollaborative, communicative: Regular check-ins and draft reviews keep us aligned."
  },
  {
    "objectID": "consulting.html#strategic-research-scalable-data-science-trustworthy-results",
    "href": "consulting.html#strategic-research-scalable-data-science-trustworthy-results",
    "title": "Consulting Services",
    "section": "",
    "text": "I partner with teams to design and execute rigorous, reproducible research—combining advanced statistical methods, applied data science, and robust study design to inform decision-making.\n\n\n\n\nOr email me directly at consulting@johnflournoy.science\n\n\n \n\nName \n\n\nEmail \n\n\nMessage\n\n\n\n\n\nSend Message"
  },
  {
    "objectID": "consulting.html#core-expertise",
    "href": "consulting.html#core-expertise",
    "title": "Consulting Services",
    "section": "",
    "text": "I earned my PhD in Psychology at the University of Oregon, completed post-doctoral work at Harvard where I was hired on as a Research Associate, and most recently led technologist-focused science as Principal Research Scientist at the Developer Success Lab. I’m available to bring that same rigor directly to your projects.\n\nStatistical Methods & Modeling\n\nHierarchical Bayesian models, multilevel & SEM frameworks\nSimulation studies, causal inference, power analyses\nReproducible pipelines in R, Stan, Python, SQL\n\nResearch & Study Design\n\nExperimental, observational, quasi-experimental, and mixed-methods approaches\nPre-registration, sampling frameworks, measurement validation\nIntegrated qualitative insights\nSurvey-embedded experiments\n\nSurveys & Instrument Development\n\nQuestion wording, scale construction, psychometric evaluation\nSampling strategies and weighting for representativeness\nMissing-data handling, data cleaning\n\nApplied Qualitative Integration\n\nQualitative interviewing\nThematic transcript coding\nQuantitative natural language processing"
  },
  {
    "objectID": "consulting.html#engagement-model",
    "href": "consulting.html#engagement-model",
    "title": "Consulting Services",
    "section": "",
    "text": "All work is package-based—we collaborate on a plan that includes a block of hours each month for live collaboration (video calls, async chat) and all deliverables (analysis scripts, study plans, slide decks). Packages scale from a quick sanity check to an accelerated, end-to-end research sprint."
  },
  {
    "objectID": "consulting.html#what-i-bring-to-your-team",
    "href": "consulting.html#what-i-bring-to-your-team",
    "title": "Consulting Services",
    "section": "",
    "text": "Broad research experience: 15+ years across multiple methods and domains of inquiry\nTransparent workflows: Code and analyses shared via version control with clear documentation and we work together to make sure everything is reproducible.\nCrafted for your audience: Reports, slide decks, and visuals are created to be understood by stakeholders with a variety of expertise.\nCollaborative, communicative: Regular check-ins and draft reviews keep us aligned."
  },
  {
    "objectID": "trophy.html",
    "href": "trophy.html",
    "title": "silly trophies",
    "section": "",
    "text": "trophy"
  },
  {
    "objectID": "cv_page.html",
    "href": "cv_page.html",
    "title": "CV",
    "section": "",
    "text": "Download it"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download it"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "John C. Flournoy, PhD",
    "section": "",
    "text": "I am a research scientist with a focus on statistical methodology, psychometrics, and research computing infrastructure. I have published on adolescent health and development, and more recently driven business strategy with insights about software developers and other technologists. My unique perspective and skillset comes from:\n\nDeep methodological and empirical rigor (which has primarily meant statistical rigor to me), with particular strength in Bayesian modeling using Stan and brms in R\nA PhD in psychology and background as a (reformed fMRI) neuroscientist, specializing in adolescent development, stress, and well-being\nA strong focus on both empirical and conceptual rigor, integrating quantitative methods with philosophical frameworks and qualitative discovery\n\nMy research approach stands out through:\n\nAdvanced statistical modeling and uncertainty quantification across diverse data sources - from traditional surveys and experiments to incidental observational data like Jira tickets (in forthcoming work)\nExtensive experience with longitudinal data at various cadences\nDeep commitment to construct validity and philosophical foundations, shaped profoundly by works like Alexandrova’s “A Philosophy for the Science of Well-Being”.\n\nAlexandrova, A. (2017). A philosophy for the science of well-being. Oxford university press.I strive to be effective through this intimate dialogue between our numeric approach, our theory, and our philosophy of the phenomena of interest\n\n\n\n\n\n\n\n."
  },
  {
    "objectID": "lol.html",
    "href": "lol.html",
    "title": "List of Links",
    "section": "",
    "text": "Before (good) search engines, we relied on lists of links on web pages we already knew how to find. For all of the magic an algorithmic recommendation system evinces, this is still a damn fine way to discover stuff you might like.\nStuff I’ve made or contributed to that I think you’d like to know about:\n\nThe Developer Science Review  is a collection of scholarly work curated by the former Developer Success Lab.\njflournoy/verse-cmdstan is a Docker image based on rocker/verse with cmdstan, cmdstanr, and other goodies built on top \nlocal Random Intercept Cross-Lagged Panel Model Tutorial\n\nThis is a tutorial I made a very, very, very long time ago that folks are still finding really helpful.\n\nlocal How to plan and preregister a multiverse analysis\n\nI and some good friends worked on a bunch of multiverse (aka specification-curve) analyses together and gathered our experiences and thoughts into this brief guide. It even has an awesome shiny app that Dani Cosme, PhD, made.\n\nA collection of images of maps on Unsplash I use as rotating new-tab backgrounds\n\nThere wasn’t a collection that targeted maps (and nothing else, excluding images of, e.g., people holding maps). I use the term “map” a little bit broadly to also include renderings of topographic data and e.g., needlepoint of middle-earth.\n\nlocal How hiring could distort the (lack of) association between programming performance and agreeableness\n\nEveryone loves to learn about collider bias.\n\ngiftwrapr  is an R package to make generating Docker containers for your project easy.\n\nStuff on the world wide web:\n\nIndependent Map Sellers is perhaps the best source for interesting and amazing map-based products I’ve ever come across, and I’ve spent a while looking.\n\nA project of the heart from Daniel P. Huffman who does fantastic cartography work.\n\nA list of authors and books (of fiction) that use extensive footnotes or indexing\n\nThanks to Finn Brunton for my first introduction, in one of his undergraduate essays, to the delight of footnotes.\n\nThe website of Gene Keyes, author of the Cahill-Keyes map\n\nThis is a paragon exemplar of early internet website design, and hosts a wealth of thinking on map making, such as this scathing review of the Fuller Dymaxion map\n\nMore coming soon!"
  },
  {
    "objectID": "pubs.html",
    "href": "pubs.html",
    "title": "Publications",
    "section": "",
    "text": "Chen, Y. Y., Rosenbaum, G. M., Fan, H., Flournoy, J. C., Li, T., Cegarra, L., Youssoufian, D. A., Grad-Freilich, M. J., Kordyban, L. E., Mair, P., & Somerville, L. H. (2025). Social Contexts Requiring Adjudication between Self- and Peer-Interest Differentially Alter Risk Preferences Across Adolescence. Open Mind, 9, 540–558. https://doi.org/10.1162/opmi_a_00201\n\n\nFlournoy, J. C., Lee, C. S., Wu, M., & Hicks, C. M. (2025). No silver bullets: Why understanding software cycle time is messy, not magic. Empirical Software Engineering, 30(6), 174. https://doi.org/10.1007/s10664-025-10735-w\n\n\nGuazzelli Williamson, V., Barendse, M. E. A., Chavez, S. J., Flournoy, J. C., Cheng, T. W., Cosme, D., Byrne, M. L., Allen, N. B., & Pfeifer, J. H. (2025). A longitudinal neuroimaging study of adolescent girls’ mentalizing and perspective-taking tendencies. Developmental Cognitive Neuroscience, 72, 101526. https://doi.org/10.1016/j.dcn.2025.101526\n\n\nRakesh, D., Flournoy, J. C., & McLaughlin, K. A. (2025). Associations between socioeconomic status and mental health trajectories during early adolescence: Findings from the Adolescent Brain Cognitive Development study. JCPP Advances, 5(4), e70001. https://doi.org/10.1002/jcv2.70001\n\n\nChen, Y. Y., Rosenbaum, G. M., Fan, H., Li, T., Flournoy, J., Mair, P., & Somerville, L. (2024). How beliefs around peers’ risk preferences get incorporated into adolescents’ decision making. Proceedings of the Annual Meeting of the Cognitive Science Society, 46(0). https://escholarship.org/uc/item/6c42379z\n\n\nCurtis, M., Flournoy, J. C., Kandala, S., Sanders, A. F. P., Harms, M. P., Omary, A., Somerville, L. H., & Barch, D. M. (2024). Disentangling the unique contributions of age, pubertal stage, and pubertal hormones to brain structure in childhood and adolescence. Developmental Cognitive Neuroscience, 70, 101473. https://doi.org/10.1016/j.dcn.2024.101473\n\n\nFlournoy, J. C., Bryce, N. V., Dennison, M. J., Rodman, A. M., McNeilly, E. A., Lurie, L. A., Bitran, D., Reid-Russell, A., Vidal Bustamante, C. M., Madhyastha, T., & McLaughlin, K. A. (2024). A precision neuroscience approach to estimating reliability of neural responses during emotion processing: Implications for task-fMRI. NeuroImage, 285, 120503. https://doi.org/10.1016/j.neuroimage.2023.120503\n\n\nHeffer, T., Flournoy, J. C., Baum, G. L., & Somerville, L. H. (2024). Examining the Association between Punishment and Reward Sensitivity and Response Inhibition to Previously-Incentivized Cues across Development. Journal of Youth and Adolescence, 53(6), 1341–1354. https://doi.org/10.1007/s10964-024-01966-z\n\n\nRith-Najarian, L. R., Gong-Guy, E., Flournoy, J. C., & Chavira, D. A. (2024). Randomized controlled trial of a web-based program for preventing anxiety and depression in university students. Journal of Consulting and Clinical Psychology, 92(1), 1–15. https://doi.org/10.1037/ccp0000843\n\n\nXu, B., Dall’Aglio, L., Flournoy, J., Bortsova, G., Tervo-Clemmens, B., Collins, P., de Bruijne, M., Luciana, M., Marquand, A., Wang, H., Tiemeier, H., & Muetzel, R. L. (2024). Limited generalizability of multivariate brain-based dimensions of child psychiatric symptoms. Communications Psychology, 2(1), 1–14. https://doi.org/10.1038/s44271-024-00063-y\n\n\nByrne, M. L., Vijayakumar, N., Chavez, S. J., Flournoy, J. C., Cheng, T. W., Mills, K. L., Barendse, M. E. A., Mobasser, A., Flannery, J. E., Nelson, B. W., Wang, W., Shirtcliff, E. A., Allen, N. B., & Pfeifer, J. H. (2023). Associations between multi-method latent factors of puberty and brain structure in adolescent girls. Developmental Cognitive Neuroscience, 60, 101228. https://doi.org/10.1016/j.dcn.2023.101228\n\n\nGrisanzio, K. A., Flournoy, J. C., Mair, P., & Somerville, L. H. (2023). Shifting qualities of negative affective experience through adolescence: Age-related change and associations with functional outcomes. Emotion, 23(1), 278–288. https://doi.org/10.1037/emo0001079\n\n\nMcCormick, E. M., Byrne, M. L., Flournoy, J. C., Mills, K. L., & Pfeifer, J. H. (2023). The Hitchhiker’s guide to longitudinal models: A primer on model selection for repeated-measures methods. Developmental Cognitive Neuroscience, 63, 101281. https://doi.org/10.1016/j.dcn.2023.101281\n\n\nMcLaughlin, K. A., Weissman, D. G., & Flournoy, J. (2023). Challenges with Latent Variable Approaches to Operationalizing Dimensions of Childhood adversity – a Commentary on Sisitsky et al. (2023). Research on Child and Adolescent Psychopathology, 51(12), 1809–1811. https://doi.org/10.1007/s10802-023-01114-4\n\n\nXu, B., Dall’Aglio, L., Flournoy, J., Bortsova, G., Tervo-Clemmens, B., Collins, P., Bruijne, M. de, Luciana, M., Marquand, A., Wang, H., Tiemeier, H., & Muetzel, R. L. (2023, March 20). Multivariate brain-based dimensions of child psychiatric problems: Degrees of generalizability. https://doi.org/10.1101/2023.03.12.23287158 (Pre-published)\n\n\nBarendse, M. E. A., Byrne, M. L., Flournoy, J. C., McNeilly, E. A., Guazzelli Williamson, V., Barrett, A.-M. Y., Chavez, S. J., Shirtcliff, E. A., Allen, N. B., & Pfeifer, J. H. (2022). Multimethod assessment of pubertal timing and associations with internalizing psychopathology in early adolescent girls. Journal of Psychopathology and Clinical Science, 131(1), 14–25. https://doi.org/10.1037/abn0000721\n\n\nBaum, G. L., Flournoy, J. C., Glasser, M. F., Harms, M. P., Mair, P., Sanders, A. F. P., Barch, D. M., Buckner, R. L., Bookheimer, S., Dapretto, M., Smith, S., Thomas, K. M., Yacoub, E., Essen, D. C. V., & Somerville, L. H. (2022). Graded Variation in T1w/T2w Ratio during Adolescence: Measurement, Caveats, and Implications for Development of Cortical Myelin. Journal of Neuroscience, 42(29), 5681–5694. https://doi.org/10.1523/JNEUROSCI.2380-21.2022\n\n\nByrne, M., Allen, N. B., Barendse, M. E. A., Barrett, A.-M., Chavez, S. J., Cheng, T., Flournoy, J. C., McNeilly, E. A., Shirtcliff, E. A., Vijayakumar, N., Williamson, V. G., & Pfeifer, J. (2022). Pubertal Hormones as a Measurement Method of Pubertal Stage and Timing Related to Internalizing Psychopathology in Adolescent Girls. Biological Psychiatry, 91(9), S64–S65. https://doi.org/10.1016/j.biopsych.2022.02.180\n\n\nCosme, D., Flournoy, J. C., Livingston, J. L., Lieberman, M. D., Dapretto, M., & Pfeifer, J. H. (2022). Testing the adolescent social reorientation model during self and other evaluation using hierarchical growth curve modeling with parcellated fMRI data. Developmental Cognitive Neuroscience, 54, 101089. https://doi.org/10.1016/j.dcn.2022.101089\n\n\nNelson, B. W., Flannery, J. E., Flournoy, J., Duell, N., Prinstein, M. J., & Telzer, E. (2022). Concurrent and prospective associations between fitbit wearable-derived RDoC arousal and regulatory constructs and adolescent internalizing symptoms. Journal of Child Psychology and Psychiatry, 63(3), 282–295. https://doi.org/10.1111/jcpp.13471\n\n\nRomeo, R. R., Flournoy, J. C., McLaughlin, K. A., & Lengua, L. J. (2022). Language development as a mechanism linking socioeconomic status to executive functioning development in preschool. Developmental Science, 25(5), e13227. https://doi.org/10.1111/desc.13227\n\n\nVijayakumar, N., Cheng, T. W., Flannery, J. E., Flournoy, J. C., Ross, G., Mobasser, A., Op de Macks, Z., Fisher, P. A., & Pfeifer, J. H. (2022). Differential neural sensitivity to social inclusion and exclusion in adolescents in foster care. NeuroImage: Clinical, 34, 102986. https://doi.org/10.1016/j.nicl.2022.102986\n\n\nBryce, N. V., Flournoy, J. C., Guassi Moreira, J. F., Rosen, M. L., Sambook, K. A., Mair, P., & McLaughlin, K. A. (2021). Brain parcellation selection: An overlooked decision point with meaningful effects on individual differences in resting-state functional connectivity. NeuroImage, 243, 118487. https://doi.org/10.1016/j.neuroimage.2021.118487\n\n\nGau, R., Noble, S., Heuer, K., Bottenhorn, K. L., Bilgin, I. P., Yang, Y.-F., Huntenburg, J. M., Bayer, J. M. M., Bethlehem, R. A. I., Rhoads, S. A., Vogelbacher, C., Borghesani, V., Levitis, E., Wang, H.-T., Van Den Bossche, S., Kobeleva, X., Legarreta, J. H., Guay, S., Atay, S. M., … Zuo, X.-N. (2021). Brainhack: Developing a culture of open, inclusive, community-driven neuroscience. Neuron. https://doi.org/10.1016/j.neuron.2021.04.001\n\n\nHughes, B. T., Flournoy, J. C., & Srivastava, S. (2021). Is perceived similarity more than assumed similarity? An interpersonal path to seeing similarity between self and others. Journal of Personality and Social Psychology, 121(1), 184–200. https://doi.org/10.1037/pspp0000369\n\n\nLópez-Vicente, M., Agcaoglu, O., Pérez-Crespo, L., Estévez-López, F., Heredia-Genestar, J. M., Mulder, R. H., Flournoy, J. C., van Duijvenvoorde, A. C. K., Güroğlu, B., White, T., Calhoun, V., Tiemeier, H., & Muetzel, R. L. (2021). Developmental Changes in Dynamic Functional Connectivity From Childhood Into Adolescence. Frontiers in Systems Neuroscience, 15. https://doi.org/10.3389/fnsys.2021.724805\n\n\nNook, E. C., Flournoy, J. C., Rodman, A. M., Mair, P., & McLaughlin, K. A. (2021). High Emotion Differentiation Buffers Against Internalizing Symptoms Following Exposure to Stressful Life Events in Adolescence: An Intensive Longitudinal Study. Clinical Psychological Science, 2167702620979786. https://doi.org/10.1177/2167702620979786\n\n\nRodman, A. M., Vidal Bustamante, C. M., Dennison, M. J., Flournoy, J. C., Coppersmith, D. D. L., Nook, E. C., Worthington, S., Mair, P., & McLaughlin, K. A. (2021). A Year in the Social Life of a Teenager: Within-Persons Fluctuations in Stress, Phone Communication, and Anxiety and Depression. Clinical Psychological Science, 2167702621991804. https://doi.org/10.1177/2167702621991804\n\n\nFlournoy, J. (2021, September 16). A comment on Ellwood-Lowe et al. (2020/2021): Presentation and discussion of results should take the lead from the preregistration. https://doi.org/10.31234/osf.io/23qsw\n\n\nAndrews, J. L., Mills, K. L., Flournoy, J. C., Flannery, J. E., Mobasser, A., Ross, G., Durnin, M., Peake, S., Fisher, P. A., & Pfeifer, J. H. (2020). Expectations of Social Consequences Impact Anticipated Involvement in Health-Risk Behavior During Adolescence. Journal of Research on Adolescence: The Official Journal of the Society for Research on Adolescence, 30(4), 1008–1024. https://doi.org/10.1111/jora.12576\n\n\nBarendse, M. E. A., Cosme, D., Flournoy, J. C., Vijayakumar, N., Cheng, T. W., Allen, N. B., & Pfeifer, J. H. (2020). Neural correlates of self-evaluation in relation to age and pubertal development in early adolescent girls. Developmental Cognitive Neuroscience, 44, 100799. https://doi.org/10.1016/j.dcn.2020.100799\n\n\nBarendse, M. E. A., Vijayakumar, N., Byrne, M. L., Flannery, J. E., Cheng, T. W., Flournoy, J. C., Nelson, B. W., Cosme, D., Mobasser, A., Chavez, S. J., Hval, L., Brady, B., Nadel, H., Helzer, A., Shirtcliff, E. A., Allen, N. B., & Pfeifer, J. H. (2020). Study Protocol: Transitions in Adolescent Girls (TAG). Frontiers in Psychiatry, 10. https://doi.org/10.3389/fpsyt.2019.01018\n\n\nCheng, T. W., Vijayakumar, N., Flournoy, J. C., Op de Macks, Z., Peake, S. J., Flannery, J. E., Mobasser, A., Alberti, S. L., Fisher, P. A., & Pfeifer, J. H. (2020). Feeling left out or just surprised? Neural correlates of social exclusion and overinclusion in adolescence. Cognitive, Affective, & Behavioral Neuroscience, 20(2), 340–355. https://doi.org/10.3758/s13415-020-00772-x\n\n\nFlournoy, J. C., Vijayakumar, N., Cheng, T. W., Cosme, D., Flannery, J. E., & Pfeifer, J. H. (2020). Improving practices and inferences in developmental cognitive neuroscience. Developmental Cognitive Neuroscience, 45, 100807. https://doi.org/10.1016/j.dcn.2020.100807\n\n\nVidal Bustamante, C. M., Rodman, A. M., Dennison, M. J., Flournoy, J. C., Mair, P., & McLaughlin, K. A. (2020). Within-person fluctuations in stressful life events, sleep, and anxiety and depression symptoms during adolescence: A multiwave prospective study. Journal of Child Psychology and Psychiatry, 61(10), 1116–1125. https://doi.org/10.1111/jcpp.13234\n\n\nVijayakumar, N., Flournoy, J. C., Mills, K. L., Cheng, T. W., Mobasser, A., Flannery, J. E., Allen, N. B., & Pfeifer, J. H. (2020). Getting to know me better: An fMRI study of intimate and superficial self-disclosure to friends during adolescence. Journal of Personality and Social Psychology, 118(5), 885–899. https://doi.org/10.1037/pspa0000182\n\n\nByrne, M. L., Chavez, S., Vijayakumar, N., Cheng, T. W., Flournoy, J., Barendse, M., Shirtcliff, E. A., Allen, N., & Pfeifer, J. (2019). Multi-method confirmatory factor analyses of puberty in early adolescent girls. https://doi.org/10.31234/osf.io/pue6f\n\n\nFlournoy, J. (2019). Adolescent Social Motives: Measurement and Implications. https://scholarsbank.uoregon.edu/xmlui/handle/1794/24216\n\n\nLudwig, R. M., Flournoy, J. C., & Berkman, E. T. (2019). Inequality in personality and temporal discounting across socioeconomic status? Assessing the evidence. Journal of Research in Personality, 81, 79–87. https://doi.org/10.1016/j.jrp.2019.05.003\n\n\nThalmayer, A. G., Saucier, G., Flournoy, J. C., & Srivastava, S. (2019). Ethics-Relevant Values as Antecedents of Personality Change: Longitudinal Findings from the Life and Time Study. Collabra: Psychology, 5(52). https://doi.org/10.1525/collabra.244\n\n\nThalmayer, A. G., Saucier, G., Srivastava, S., Flournoy, J. C., & Costello, C. K. (2019). Ethics-relevant values in adulthood: Longitudinal findings from the life and time study. Journal of Personality, 87(6), 1119–1135. https://doi.org/10.1111/jopy.12462\n\n\nVijayakumar, N., Pfeifer, J. H., Flournoy, J. C., Hernandez, L. M., & Dapretto, M. (2019). Affective reactivity during adolescence: Associations with age, puberty and testosterone. Cortex, 117, 336–350. https://doi.org/10.1016/j.cortex.2019.04.024\n\n\nVijayakumar, N., Shirtcliff, E. A., Byrne, M. L., Mills, K., Cheng, T. W., Mobasser, A., Flannery, J., Nelson, B., Flournoy, J., Wang, W., Allen, N., & Pfeifer, J. (2019). Pubertal hormones and brain structure: Exploring the value of hair assays. https://doi.org/10.31234/osf.io/btf4y\n\n\nInsel, C., & Somerville, L. H. (2018). Asymmetric neural tracking of gain and loss magnitude during adolescence. Social Cognitive and Affective Neuroscience, 13(8), 785–796. https://doi.org/10.1093/scan/nsy058\n\n\nKing, K. M., Littlefield, A. K., McCabe, C. J., Mills, K. L., Flournoy, J., & Chassin, L. (2018). Longitudinal modeling in developmental neuroimaging research: Common challenges, and solutions from developmental psychology. Developmental Cognitive Neuroscience, 33, 54–72. https://doi.org/10.1016/j.dcn.2017.11.009\n\n\nMadhyastha, T., Peverill, M., Koh, N., McCabe, C., Flournoy, J., Mills, K., King, K., Pfeifer, J., & McLaughlin, K. A. (2018). Current methods and limitations for longitudinal fMRI analysis across development. Developmental Cognitive Neuroscience, 33, 118–128. https://doi.org/10.1016/j.dcn.2017.11.006\n\n\nMatta, T. H., Flournoy, J. C., & Byrne, M. L. (2018). Making an unknown unknown a known unknown: Missing data in longitudinal neuroimaging studies. Developmental Cognitive Neuroscience, 33, 83–98. https://doi.org/10.1016/j.dcn.2017.10.001\n\n\nOp de Macks, Z. A., Flannery, J. E., Peake, S. J., Flournoy, J. C., Mobasser, A., Alberti, S. L., Fisher, P. A., & Pfeifer, J. H. (2018). Novel insights from the Yellow Light Game: Safe and risky decisions differentially impact adolescent outcome-related brain function. NeuroImage, 181, 568–581. https://doi.org/10.1016/j.neuroimage.2018.06.058\n\n\nFlannery, J. E., Giuliani, N. R., Flournoy, J. C., & Pfeifer, J. H. (2017). Neurodevelopmental changes across adolescence in viewing and labeling dynamic peer emotions. Developmental Cognitive Neuroscience, 25, 113–127. https://doi.org/10.1016/j.dcn.2017.02.003\n\n\nGiuliani, N. R., Flournoy, J. C., Ivie, E. J., Hippel, A. V., & Pfeifer, J. H. (2017). Presentation and validation of the DuckEES child and adolescent dynamic facial expressions stimulus set. International Journal of Methods in Psychiatric Research, 26(1), e1553. https://doi.org/10.1002/mpr.1553\n\n\nLeonard, J., Flournoy, J. C., Angeles, C. P. L. los, & Whitaker, K. (2017). How much motion is too much motion? Determining motion thresholds by sample size for reproducibility in developmental resting-state MRI. Research Ideas and Outcomes, 3, e12569. https://doi.org/10.3897/rio.3.e12569\n\n\nFlannery, J., Peake, S. J., Flournoy, J. C., Alberti, S. L., Mobasser, A., Fisher, P. A., & Pfeifer, J. H. (2016). Neural Outcomes of Risk Decision: Prediction of Subsequent Behavioral Decision Across Social Contexts in Adolescence. Annual Social Affective Neuroscience Conference.\n\n\nFlournoy, J. C., Pfeifer, J. H., Moore, W. E., Tackman, A. M., Masten, C. L., Mazziotta, J. C., Iacoboni, M., & Dapretto, M. (2016). Neural Reactivity to Emotional Faces May Mediate the Relationship Between Childhood Empathy and Adolescent Prosocial Behavior. Child Development, 87(6), 1691–1702. https://doi.org/10.1111/cdev.12630\n\n\nPfeifer, J. H., & Flournoy, J. C. (2016, April). Characterizing longitudinal change in brain function: Some problems and solutions. Modeling Developmental Change, Eugene, OR.\n\n\nFlannery, J. E., Giuliani, N., Flournoy, J. C., & Pfeifer, J. H. (2015). Developmental Changes in Affect Processing: Brain Functionand Connectivityto Peer Faces. 8th Annual Meeting of the Social Affective Neuroscience Society Annual Meeting.\n\n\nFlannery, J., Peake, S. J., Flournoy, J. C., Alberti, S. L., Mobasser, A., Fisher, P. A., & Pfeifer, J. H. (2015). Positive and Negative Neural Feedback Processing of Risk Decisions Across Social Contexts in Adolescents. 3rd Annual Flux Congress.\n\n\nFlournoy, J. C., Peake, S. J., Alberti, S. L., Flannery, J. E., Mobasser, A., Fisher, P. A., & Pfeifer, J. H. (2015). Evaluation of a Bayesian cognitive model for adolescent risky decision making in the Stoplight Game. Flux Congress. https://doi.org/10.13140/RG.2.1.1925.7049\n\n\nFlournoy, J. C., Srivastava, S., & Saucier, G. (2015). A Network Approach to Longitudinal Personality Change. Association for Research in Personality, 4th Biennial Meeting.\n\n\nGoodkind, M. S., Gallagher-Thompson, D., Thompson, L. W., Kesler, S. R., Anker, L., Flournoy, J. C., Berman, M. P., Holland, J. M., & O’Hara, R. M. (2015). The impact of executive function on response to cognitive behavioral therapy in late-life depression. International Journal of Geriatric Psychiatry, n/a–n/a. https://doi.org/10.1002/gps.4325\n\n\nTackman, A. M., Srivastava, S., Flournoy, J. C., & Saucier, G. (2015, February). Relationships and personality development in adulthood: A multiple-informant approach. 16th annual meeting of the Society of Personality and Social Psychology, Long Beach, CA.\n\n\nFlannery, J., Giuliani, N., Flournoy, J. C., & Pfeifer, J. H. (2015, April). Affective Reactivity and Regulation Across Adolescence: Neural Responses to Dynamic Peer Emotions. Social Affective Neuroscience Society Annual Meeting.\n\n\nFlournoy, J. C., Giuliani, N., Hill, A., Moore III, W. E., & Pfeifer, J. H. (2014). Labeling Emotional Faces Changes Affective Processing. The Society for Research on Adolescence 15th Biennial Meeting.\n\n\nFlournoy, J. C., Srivastava, S., Dapretto, M., & Pfeifer, J. H. (2014). Correlated Change in Components of Empathy During Adolescence. The Society for Personality and Social Psychology 15th Annual Meeting.\n\n\nFlournoy, J. C., Tackman, A., Moore III, W. E., Pfeifer, J. H., Mazziotta, J. C., Iacoboni, M., & Dapretto, M. (2014). Effect of Empathy on Prosocial Behavior Mediated by Brain Across the Transition into Adolescence. Social & Affective Neuroscience Society 7th Annual Meeting.\n\n\nFlournoy, J. C., Moore III, W. E., & Pfeifer, J. H. (2014, September 11). Using fMRI to study child and adolescent development: Thoughts from the trenches. Special Meeting of the Society for Research in Child Development on Developmental Methodology, San Diego, CA.\n\n\nFlannery, J., Giuliani, N., Flournoy, J. C., & Pfeifer, J. H. (2014, November). Dynamic Neuropsychological Emotion Regulation Profiles Among Youth, 10-22 Year Old. International Society of Developmental Psychobiology.\n\n\nFlournoy, J. C., Moore III, W. E., Srivastava, S., Mazziotta, J. C., Iacoboni, M., Dapretto, M., & Pfeifer, J. H. (2013, April 13). Neural Correlates of Empathy Development During Early Adolescence. Social & Affective Neuroscience Society 6th Annual Meeting.\n\n\nO’Hara, R., Marcus, P., Thompson, W. K., Flournoy, J. C., Vahia, I., Lin, X., Hallmayer, J., Depp, C., & Jeste, D. V. (2012). 5-HTTLPR Short Allele, Resilience, and Successful Aging in Older Adults. American Journal of Geriatric Psych, 20(5), 452. https://doi.org/10.1097/JGP.0b013e31823e2d03\n\n\nFlournoy, J. C., Weinkam, T., Hallmayer, J., & O’Hara, R. (2011, April 4). Anxiety, Sleep Efficiency, and 5-HTTLPR Genotype in Older Adults. Cognitive Neuroscience Society 18th Annual Meeting.\n\n\nChen, E., Anker, L., Flournoy, J. C., Kovachy, B., Blaisdell, K., Weinkam, T., Stang, B., Hawkins, N., & O’Hara, R. (2011, November). Comparison of Amount of REM Sleep and Cognitive Abilities in Normal Older Adults and Those with Sleep Apnea. International College Of Geriatric Psychoneuropharmacology 11th Annual Meeting.\n\n\nMarcus, P., Thompson, W. K., Flournoy, J. C., Vahia, I., Lin, X., Hallmayer, J., Depp, C., & O’Hara, R. (2010, September). 5-HTTLPR Short allele is Associated with Poorer Self-Perceptions of Successful Aging in Older Adults. International College Of Geriatric Psychoneuropharmacology 10th Annual Meeting."
  },
  {
    "objectID": "riclpm-lavaan-demo.html",
    "href": "riclpm-lavaan-demo.html",
    "title": "A better cross-lagged panel model, from Hamaker et al. (2015)",
    "section": "",
    "text": "Note\n\n\n\nHey! While you’re here, you should know that I’m looking for new consulting contracts and employment opportunities. Feel free to reach out if you’d like to chat about how I can help your team!\nUpdate, 2019-11-11: There are a couple of new bits of code online that could be helpful if you are interested in these models.\nThis walk-through explains, briefly, why and how to run a RI-CLPM in R.\nAll code and data are available in the github repository"
  },
  {
    "objectID": "riclpm-lavaan-demo.html#some-data",
    "href": "riclpm-lavaan-demo.html#some-data",
    "title": "A better cross-lagged panel model, from Hamaker et al. (2015)",
    "section": "Some data",
    "text": "Some data\nNow, we need some data. I’m using a data set presented on at a methods symposium at SRCD in 1997. Supporting documentation can be found in this pdf. Data and code for importing it was helpfully provided by Sanjay Srivastava.\nThe variables we’re considering are a measure of antisocial behavior (anti) and reading recognition (read). See the docs for descriptions of the other variables. And for the purpose of the model fitting below, x &lt;- anit and y &lt;- read. Following are some descriptions of the raw data:\nantiread &lt;- read.table(\"srcddata.dat\",\n                       na.strings = c(\"999.00\"),\n                       col.names = c(\"anti1\", \"anti2\", \"anti3\", \"anti4\", \n                                     \"read1\", \"read2\", \"read3\", \"read4\",\n                                     \"gen\", \"momage\", \"kidage\", \"homecog\", \n                                     \"homeemo\", \"id\")\n) %&gt;%\n  rename(x1 = anti1, x2 = anti2, x3 = anti3, x4 = anti4,\n         y1 = read1, y2 = read2, y3 = read3, y4 = read4) %&gt;%\n  select(matches('[xy][1-4]'))\n\nknitr::kable(summary(antiread), format = 'markdown')\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nx1\nx2\nx3\nx4\ny1\ny2\ny3\ny4\n\n\n\n\n\nMin. :0.000\nMin. : 0.000\nMin. : 0.000\nMin. : 0.000\nMin. :0.100\nMin. :1.600\nMin. :2.200\nMin. :2.500\n\n\n\n1st Qu.:0.000\n1st Qu.: 0.000\n1st Qu.: 0.000\n1st Qu.: 0.000\n1st Qu.:1.800\n1st Qu.:3.300\n1st Qu.:4.200\n1st Qu.:4.925\n\n\n\nMedian :1.000\nMedian : 1.500\nMedian : 1.000\nMedian : 1.500\nMedian :2.300\nMedian :4.100\nMedian :5.000\nMedian :5.800\n\n\n\nMean :1.662\nMean : 2.027\nMean : 1.828\nMean : 2.061\nMean :2.523\nMean :4.076\nMean :5.005\nMean :5.774\n\n\n\n3rd Qu.:3.000\n3rd Qu.: 3.000\n3rd Qu.: 3.000\n3rd Qu.: 3.000\n3rd Qu.:3.000\n3rd Qu.:4.900\n3rd Qu.:5.800\n3rd Qu.:6.675\n\n\n\nMax. :9.000\nMax. :10.000\nMax. :10.000\nMax. :10.000\nMax. :7.200\nMax. :8.200\nMax. :8.400\nMax. :8.300\n\n\n\nNA\nNA’s :31\nNA’s :108\nNA’s :111\nNA\nNA’s :30\nNA’s :130\nNA’s :135\n\n\n\nantiread %&gt;%\n  select(-x4,-y4) %&gt;%\n  mutate(pid = 1:n()) %&gt;%\n  gather(key, value, -pid) %&gt;%\n  extract(col = key, into = c('var', 'wave'), regex = '(\\\\w)(\\\\d)') %&gt;%\n  ggplot(aes(x = value)) +\n  geom_density(alpha = 1) + \n  facet_grid(wave~var, scales = 'free') + \n  theme_classic()\n## Warning: Removed 299 rows containing non-finite values (stat_density).\n\n\n\ncenter\n\n\nantireadLong &lt;- antiread %&gt;%\n  select(-x4,-y4) %&gt;%\n  mutate(pid = 1:n()) %&gt;%\n  gather(key, value, -pid) %&gt;%\n  extract(col = key, into = c('var', 'wave'), regex = '(\\\\w)(\\\\d)')\n\nantireadLong %&gt;%\n  ggplot(aes(x = wave, y = value, color = var, group = var)) +\n  geom_point(position = position_jitter(w = .2), alpha = .1) +\n  geom_line(stat = 'identity', aes(group = interaction(var, pid)), alpha = .04) + \n  geom_line(stat = 'smooth', method = 'lm', size = 1) + \n  theme_classic()\n## Warning: Removed 299 rows containing non-finite values (stat_smooth).\n## Warning: Removed 299 rows containing missing values (geom_point).\n## Warning: Removed 271 rows containing missing values (geom_path).\n\n\n\ncenter"
  },
  {
    "objectID": "riclpm-lavaan-demo.html#fitting-a-ri-clpm",
    "href": "riclpm-lavaan-demo.html#fitting-a-ri-clpm",
    "title": "A better cross-lagged panel model, from Hamaker et al. (2015)",
    "section": "Fitting a RI-CLPM",
    "text": "Fitting a RI-CLPM\nIn the below lavaan code, I’ll be using the notation from the diagram. I am explicitly specifying everything in the diagram, which is why in the call to lavaan I set a bunch of auto options to false. This is because often lavaan will try to automatically estimate things that you don’t usually write out but often want estimated, like residuals. Because this model is unorthodox, I want to be as explicit as possible.\nThe lavaan code below uses syntax that can be found in their help docs for the basic stuff as well as the more advanced labeling and constraining.\nriclpmModel &lt;- \n'\n#Note, the data contain x1-3 and y1-3\n#Latent mean Structure with intercepts\n\nkappa =~ 1*x1 + 1*x2 + 1*x3\nomega =~ 1*y1 + 1*y2 + 1*y3\n\nx1 ~ mu1*1 #intercepts\nx2 ~ mu2*1\nx3 ~ mu3*1\ny1 ~ pi1*1\ny2 ~ pi2*1\ny3 ~ pi3*1\n\nkappa ~~ kappa #variance\nomega ~~ omega #variance\nkappa ~~ omega #covariance\n\n#laten vars for AR and cross-lagged effects\np1 =~ 1*x1 #each factor loading set to 1\np2 =~ 1*x2\np3 =~ 1*x3\nq1 =~ 1*y1\nq2 =~ 1*y2\nq3 =~ 1*y3\n\n#Later, we may constrain autoregression and cross-lagged\n#effects to be the same across both lags.\np3 ~ alpha3*p2 + beta3*q2\np2 ~ alpha2*p1 + beta2*q1\n\nq3 ~ delta3*q2 + gamma3*p2\nq2 ~ delta2*q1 + gamma2*p1\n\np1 ~~ p1 #variance\np2 ~~ u2*p2\np3 ~~ u3*p3\nq1 ~~ q1 #variance\nq2 ~~ v2*q2\nq3 ~~ v3*q3\n\np1 ~~ q1 #p1 and q1 covariance\np2 ~~ q2 #p2 and q2 covariance\np3 ~~ q3 #p2 and q2 covariance'\n\nfit &lt;- lavaan(riclpmModel, data = antiread,\n              missing = 'ML', #for the missing data!\n              int.ov.free = F,\n              int.lv.free = F,\n              auto.fix.first = F,\n              auto.fix.single = F,\n              auto.cov.lv.x = F,\n              auto.cov.y = F,\n              auto.var = F)\nsummary(fit, standardized = T)\n## lavaan (0.5-23.1097) converged normally after  83 iterations\n## \n##   Number of observations                           405\n## \n##   Number of missing patterns                        14\n## \n##   Estimator                                         ML\n##   Minimum Function Test Statistic                3.213\n##   Degrees of freedom                                 1\n##   P-value (Chi-square)                           0.073\n## \n## Parameter Estimates:\n## \n##   Information                                 Observed\n##   Standard Errors                             Standard\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   kappa =~                                                              \n##     x1                1.000                               1.110    0.671\n##     x2                1.000                               1.110    0.548\n##     x3                1.000                               1.110    0.571\n##   omega =~                                                              \n##     y1                1.000                               0.734    0.794\n##     y2                1.000                               0.734    0.679\n##     y3                1.000                               0.734    0.622\n##   p1 =~                                                                 \n##     x1                1.000                               1.226    0.741\n##   p2 =~                                                                 \n##     x2                1.000                               1.692    0.836\n##   p3 =~                                                                 \n##     x3                1.000                               1.594    0.821\n##   q1 =~                                                                 \n##     y1                1.000                               0.562    0.608\n##   q2 =~                                                                 \n##     y2                1.000                               0.794    0.734\n##   q3 =~                                                                 \n##     y3                1.000                               0.924    0.783\n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   p3 ~                                                                  \n##     p2      (alp3)    0.353    0.078    4.549    0.000    0.374    0.374\n##     q2      (bet3)   -0.274    0.154   -1.788    0.074   -0.137   -0.137\n##   p2 ~                                                                  \n##     p1      (alp2)    0.162    0.169    0.961    0.336    0.118    0.118\n##     q1      (bet2)   -0.090    0.457   -0.196    0.845   -0.030   -0.030\n##   q3 ~                                                                  \n##     q2      (dlt3)    0.738    0.073   10.048    0.000    0.634    0.634\n##     p2      (gmm3)   -0.008    0.034   -0.241    0.810   -0.015   -0.015\n##   q2 ~                                                                  \n##     q1      (dlt2)    0.374    0.350    1.068    0.286    0.265    0.265\n##     p1      (gmm2)   -0.058    0.079   -0.733    0.464   -0.089   -0.089\n## \n## Covariances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   kappa ~~                                                              \n##     omega            -0.118    0.157   -0.754    0.451   -0.145   -0.145\n##   p1 ~~                                                                 \n##     q1                0.017    0.160    0.107    0.915    0.025    0.025\n##  .p2 ~~                                                                 \n##    .q2               -0.117    0.114   -1.025    0.305   -0.091   -0.091\n##  .p3 ~~                                                                 \n##    .q3               -0.115    0.071   -1.624    0.104   -0.111   -0.111\n## \n## Intercepts:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##    .x1       (mu1)    1.662    0.082   20.217    0.000    1.662    1.005\n##    .x2       (mu2)    1.985    0.103   19.189    0.000    1.985    0.981\n##    .x3       (mu3)    1.898    0.107   17.658    0.000    1.898    0.977\n##    .y1       (pi1)    2.523    0.046   54.925    0.000    2.523    2.729\n##    .y2       (pi2)    4.066    0.055   74.267    0.000    4.066    3.760\n##    .y3       (pi3)    5.023    0.064   78.328    0.000    5.023    4.256\n##     kappa             0.000                               0.000    0.000\n##     omega             0.000                               0.000    0.000\n##     p1                0.000                               0.000    0.000\n##    .p2                0.000                               0.000    0.000\n##    .p3                0.000                               0.000    0.000\n##     q1                0.000                               0.000    0.000\n##    .q2                0.000                               0.000    0.000\n##    .q3                0.000                               0.000    0.000\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##     kappa             1.232    0.271    4.550    0.000    1.000    1.000\n##     omega             0.539    0.177    3.042    0.002    1.000    1.000\n##     p1                1.504    0.281    5.361    0.000    1.000    1.000\n##    .p2        (u2)    2.821    0.316    8.929    0.000    0.985    0.985\n##    .p3        (u3)    2.110    0.204   10.341    0.000    0.830    0.830\n##     q1                0.316    0.172    1.832    0.067    1.000    1.000\n##    .q2        (v2)    0.582    0.086    6.811    0.000    0.923    0.923\n##    .q3        (v3)    0.509    0.046   11.125    0.000    0.596    0.596\n##    .x1                0.000                               0.000    0.000\n##    .x2                0.000                               0.000    0.000\n##    .x3                0.000                               0.000    0.000\n##    .y1                0.000                               0.000    0.000\n##    .y2                0.000                               0.000    0.000\n##    .y3                0.000                               0.000    0.000"
  },
  {
    "objectID": "riclpm-lavaan-demo.html#comparing-fits",
    "href": "riclpm-lavaan-demo.html#comparing-fits",
    "title": "A better cross-lagged panel model, from Hamaker et al. (2015)",
    "section": "Comparing fits",
    "text": "Comparing fits\n\nRI-CLPM v CLPM\nBecause the traditional CLPM is nested in the RI-CLPM, we can compare model fit. The correct reference distribution for this comparison is \\(\\chi^2\\) but, as Hamaker and colleagues say\n{% quote hamaker_critique_2015 %} However, we can make use of the fact that the regular chi-square difference test is conservative in this context, meaning that, if it is significant, we are certain that the correct (i.e., chi-bar-square difference) test will be significant too. (p. 105) {% endquote %}\nWe estimate the traditional CLPM by setting the variance and covariance of \\(\\kappa\\) and \\(\\omega\\) to 0.\nclpmModel &lt;- #yes, \"Model\" is redundant\n'\n#Note, the data contain x1-3 and y1-3\n#Latent mean Structure with intercepts\n\nkappa =~ 1*x1 + 1*x2 + 1*x3\nomega =~ 1*y1 + 1*y2 + 1*y3\n\nx1 ~ mu1*1 #intercepts\nx2 ~ mu2*1\nx3 ~ mu3*1\ny1 ~ pi1*1\ny2 ~ pi2*1\ny3 ~ pi3*1\n\nkappa ~~ 0*kappa #variance nope\nomega ~~ 0*omega #variance nope\nkappa ~~ 0*omega #covariance not even\n\n#laten vars for AR and cross-lagged effects\np1 =~ 1*x1 #each factor loading set to 1\np2 =~ 1*x2\np3 =~ 1*x3\nq1 =~ 1*y1\nq2 =~ 1*y2\nq3 =~ 1*y3\n\np3 ~ alpha3*p2 + beta3*q2\np2 ~ alpha2*p1 + beta2*q1\n\nq3 ~ delta3*q2 + gamma3*p2\nq2 ~ delta2*q1 + gamma2*p1\n\np1 ~~ p1 #variance\np2 ~~ u2*p2\np3 ~~ u3*p3\nq1 ~~ q1 #variance\nq2 ~~ v2*q2\nq3 ~~ v3*q3\n\np1 ~~ q1 #p1 and q1 covariance\np2 ~~ q2 #p2 and q2 covariance\np3 ~~ q3 #p2 and q2 covariance'\n\nfitCLPM &lt;- lavaan(clpmModel, data = antiread,\n              missing = 'ML', #for the missing data!\n              int.ov.free = F,\n              int.lv.free = F,\n              auto.fix.first = F,\n              auto.fix.single = F,\n              auto.cov.lv.x = F,\n              auto.cov.y = F,\n              auto.var = F)\n\nanova(fit, fitCLPM)\n## Chi Square Difference Test\n## \n##         Df    AIC    BIC   Chisq Chisq diff Df diff Pr(&gt;Chisq)    \n## fit      1 6814.1 6918.2  3.2127                                  \n## fitCLPM  4 6825.6 6917.7 20.6779     17.465       3  0.0005669 ***\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nThe CLPM fits much worse, with no model comparison statistic favoring the CLPM (the BIC advantage of 1 point is negligible). We can print out the standardized estimates to compare to the unconstrained RI-CLPM above.\nsummary(fitCLPM, standardize = T)\n## lavaan (0.5-23.1097) converged normally after  47 iterations\n## \n##   Number of observations                           405\n## \n##   Number of missing patterns                        14\n## \n##   Estimator                                         ML\n##   Minimum Function Test Statistic               20.678\n##   Degrees of freedom                                 4\n##   P-value (Chi-square)                           0.000\n## \n## Parameter Estimates:\n## \n##   Information                                 Observed\n##   Standard Errors                             Standard\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   kappa =~                                                              \n##     x1                1.000                               0.000    0.000\n##     x2                1.000                               0.000    0.000\n##     x3                1.000                               0.000    0.000\n##   omega =~                                                              \n##     y1                1.000                               0.000    0.000\n##     y2                1.000                               0.000    0.000\n##     y3                1.000                               0.000    0.000\n##   p1 =~                                                                 \n##     x1                1.000                               1.656    1.000\n##   p2 =~                                                                 \n##     x2                1.000                               2.024    1.000\n##   p3 =~                                                                 \n##     x3                1.000                               1.955    1.000\n##   q1 =~                                                                 \n##     y1                1.000                               0.924    1.000\n##   q2 =~                                                                 \n##     y2                1.000                               1.082    1.000\n##   q3 =~                                                                 \n##     y3                1.000                               1.174    1.000\n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   p3 ~                                                                  \n##     p2      (alp3)    0.559    0.050   11.150    0.000    0.579    0.579\n##     q2      (bet3)   -0.145    0.085   -1.698    0.090   -0.080   -0.080\n##   p2 ~                                                                  \n##     p1      (alp2)    0.538    0.056    9.679    0.000    0.440    0.440\n##     q1      (bet2)   -0.093    0.101   -0.920    0.357   -0.042   -0.042\n##   q3 ~                                                                  \n##     q2      (dlt3)    0.849    0.042   20.402    0.000    0.783    0.783\n##     p2      (gmm3)   -0.016    0.024   -0.677    0.498   -0.028   -0.028\n##   q2 ~                                                                  \n##     q1      (dlt2)    0.763    0.045   16.903    0.000    0.651    0.651\n##     p1      (gmm2)   -0.047    0.025   -1.871    0.061   -0.072   -0.072\n## \n## Covariances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   kappa ~~                                                              \n##     omega             0.000                                 NaN      NaN\n##   p1 ~~                                                                 \n##     q1               -0.107    0.076   -1.403    0.161   -0.070   -0.070\n##  .p2 ~~                                                                 \n##    .q2               -0.085    0.077   -1.098    0.272   -0.058   -0.058\n##  .p3 ~~                                                                 \n##    .q3               -0.121    0.071   -1.710    0.087   -0.106   -0.106\n## \n## Intercepts:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##    .x1       (mu1)    1.662    0.082   20.194    0.000    1.662    1.003\n##    .x2       (mu2)    1.983    0.103   19.189    0.000    1.983    0.980\n##    .x3       (mu3)    1.902    0.109   17.503    0.000    1.902    0.973\n##    .y1       (pi1)    2.523    0.046   54.956    0.000    2.523    2.731\n##    .y2       (pi2)    4.066    0.055   74.277    0.000    4.066    3.759\n##    .y3       (pi3)    5.023    0.064   78.548    0.000    5.023    4.279\n##     kappa             0.000                                 NaN      NaN\n##     omega             0.000                                 NaN      NaN\n##     p1                0.000                               0.000    0.000\n##    .p2                0.000                               0.000    0.000\n##    .p3                0.000                               0.000    0.000\n##     q1                0.000                               0.000    0.000\n##    .q2                0.000                               0.000    0.000\n##    .q3                0.000                               0.000    0.000\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##     kappa             0.000                                 NaN      NaN\n##     omega             0.000                                 NaN      NaN\n##     p1                2.742    0.193   14.230    0.000    1.000    1.000\n##    .p2        (u2)    3.284    0.239   13.726    0.000    0.802    0.802\n##    .p3        (u3)    2.474    0.206   12.035    0.000    0.648    0.648\n##     q1                0.853    0.060   14.230    0.000    1.000    1.000\n##    .q2        (v2)    0.660    0.048   13.698    0.000    0.564    0.564\n##    .q3        (v3)    0.525    0.046   11.509    0.000    0.381    0.381\n##    .x1                0.000                               0.000    0.000\n##    .x2                0.000                               0.000    0.000\n##    .x3                0.000                               0.000    0.000\n##    .y1                0.000                               0.000    0.000\n##    .y2                0.000                               0.000    0.000\n##    .y3                0.000                               0.000    0.000\n\n\nAdding constraints to RI-CLPM\nFor parsimony, I usually try to constraint my autoregressive and cross-lagged paths to be the same across intervals. Oh, and residuals too. I’ll do this in the following code and then check the fit against the unconstrained model. To do this, all I have to do is make sure the paths have the same name, like alpha instead of alpha2 and alpha3.\nriclpmModelConstrainedARCL &lt;- \n'\n#Note, the data contain x1-3 and y1-3\n#Latent mean Structure with intercepts\n\nkappa =~ 1*x1 + 1*x2 + 1*x3\nomega =~ 1*y1 + 1*y2 + 1*y3\n\nx1 ~ mu1*1 #intercepts\nx2 ~ mu2*1\nx3 ~ mu3*1\ny1 ~ pi1*1\ny2 ~ pi2*1\ny3 ~ pi3*1\n\nkappa ~~ kappa #variance\nomega ~~ omega #variance\nkappa ~~ omega #covariance\n\n#laten vars for AR and cross-lagged effects\np1 =~ 1*x1 #each factor loading set to 1\np2 =~ 1*x2\np3 =~ 1*x3\nq1 =~ 1*y1\nq2 =~ 1*y2\nq3 =~ 1*y3\n\n#constrain autoregression and cross lagged effects to be the same across both lags.\np3 ~ alpha*p2 + beta*q2\np2 ~ alpha*p1 + beta*q1\n\nq3 ~ delta*q2 + gamma*p2\nq2 ~ delta*q1 + gamma*p1\n\np1 ~~ p1 #variance\np2 ~~ u*p2\np3 ~~ u*p3\nq1 ~~ q1 #variance\nq2 ~~ v*q2\nq3 ~~ v*q3\n\np1 ~~ q1 #p1 and q1 covariance\np2 ~~ uv*q2 #p2 and q2 covariance should also be constrained to be the same as\np3 ~~ uv*q3 #p3 and q3 covariance'\n\nfitConstrainedARCL &lt;- lavaan(riclpmModelConstrainedARCL, data = antiread,\n              missing = 'ML', #for the missing data!\n              int.ov.free = F,\n              int.lv.free = F,\n              auto.fix.first = F,\n              auto.fix.single = F,\n              auto.cov.lv.x = F,\n              auto.cov.y = F,\n              auto.var = F)\n\nanova(fit, fitConstrainedARCL)\n## Chi Square Difference Test\n## \n##                    Df    AIC    BIC   Chisq Chisq diff Df diff Pr(&gt;Chisq)\n## fit                 1 6814.1 6918.2  3.2127                              \n## fitConstrainedARCL  8 6820.2 6896.2 23.2640     20.051       7    0.00546\n##                      \n## fit                  \n## fitConstrainedARCL **\n## ---\n## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\nWell, according to AIC and the \\(\\chi^2\\) test, the constrained model fits worse. But BIC loves the constrained model because it hates parameters. Interpretive ease hates parameters too (most of the time), so let’s look at the summary for our simplified model.\nsummary(fitConstrainedARCL, standardized = T)\n## lavaan (0.5-23.1097) converged normally after  78 iterations\n## \n##   Number of observations                           405\n## \n##   Number of missing patterns                        14\n## \n##   Estimator                                         ML\n##   Minimum Function Test Statistic               23.264\n##   Degrees of freedom                                 8\n##   P-value (Chi-square)                           0.003\n## \n## Parameter Estimates:\n## \n##   Information                                 Observed\n##   Standard Errors                             Standard\n## \n## Latent Variables:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   kappa =~                                                              \n##     x1                1.000                               1.025    0.616\n##     x2                1.000                               1.025    0.525\n##     x3                1.000                               1.025    0.516\n##   omega =~                                                              \n##     y1                1.000                               0.566    0.612\n##     y2                1.000                               0.566    0.519\n##     y3                1.000                               0.566    0.484\n##   p1 =~                                                                 \n##     x1                1.000                               1.311    0.788\n##   p2 =~                                                                 \n##     x2                1.000                               1.662    0.851\n##   p3 =~                                                                 \n##     x3                1.000                               1.702    0.857\n##   q1 =~                                                                 \n##     y1                1.000                               0.732    0.791\n##   q2 =~                                                                 \n##     y2                1.000                               0.932    0.855\n##   q3 =~                                                                 \n##     y3                1.000                               1.024    0.875\n## \n## Regressions:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   p3 ~                                                                  \n##     p2      (alph)    0.306    0.092    3.331    0.001    0.299    0.299\n##     q2      (beta)   -0.212    0.148   -1.433    0.152   -0.116   -0.116\n##   p2 ~                                                                  \n##     p1      (alph)    0.306    0.092    3.331    0.001    0.241    0.241\n##     q1      (beta)   -0.212    0.148   -1.433    0.152   -0.093   -0.093\n##   q3 ~                                                                  \n##     q2      (delt)    0.713    0.076    9.391    0.000    0.648    0.648\n##     p2      (gamm)   -0.039    0.031   -1.281    0.200   -0.063   -0.063\n##   q2 ~                                                                  \n##     q1      (delt)    0.713    0.076    9.391    0.000    0.560    0.560\n##     p1      (gamm)   -0.039    0.031   -1.281    0.200   -0.055   -0.055\n## \n## Covariances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##   kappa ~~                                                              \n##     omega            -0.046    0.143   -0.321    0.748   -0.079   -0.079\n##   p1 ~~                                                                 \n##     q1               -0.056    0.146   -0.387    0.699   -0.059   -0.059\n##  .p2 ~~                                                                 \n##    .q2        (uv)   -0.116    0.060   -1.949    0.051   -0.094   -0.094\n##  .p3 ~~                                                                 \n##    .q3        (uv)   -0.116    0.060   -1.949    0.051   -0.094   -0.094\n## \n## Intercepts:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##    .x1       (mu1)    1.662    0.083   20.095    0.000    1.662    0.999\n##    .x2       (mu2)    1.990    0.100   19.944    0.000    1.990    1.019\n##    .x3       (mu3)    1.890    0.111   16.976    0.000    1.890    0.951\n##    .y1       (pi1)    2.523    0.046   54.891    0.000    2.523    2.728\n##    .y2       (pi2)    4.067    0.055   73.785    0.000    4.067    3.731\n##    .y3       (pi3)    5.018    0.064   78.033    0.000    5.018    4.290\n##     kappa             0.000                               0.000    0.000\n##     omega             0.000                               0.000    0.000\n##     p1                0.000                               0.000    0.000\n##    .p2                0.000                               0.000    0.000\n##    .p3                0.000                               0.000    0.000\n##     q1                0.000                               0.000    0.000\n##    .q2                0.000                               0.000    0.000\n##    .q3                0.000                               0.000    0.000\n## \n## Variances:\n##                    Estimate  Std.Err  z-value  P(&gt;|z|)   Std.lv  Std.all\n##     kappa             1.052    0.253    4.161    0.000    1.000    1.000\n##     omega             0.320    0.167    1.915    0.055    1.000    1.000\n##     p1                1.718    0.254    6.764    0.000    1.000    1.000\n##    .p2         (u)    2.569    0.199   12.895    0.000    0.930    0.930\n##    .p3         (u)    2.569    0.199   12.895    0.000    0.887    0.887\n##     q1                0.535    0.166    3.225    0.001    1.000    1.000\n##    .q2         (v)    0.590    0.036   16.503    0.000    0.680    0.680\n##    .q3         (v)    0.590    0.036   16.503    0.000    0.563    0.563\n##    .x1                0.000                               0.000    0.000\n##    .x2                0.000                               0.000    0.000\n##    .x3                0.000                               0.000    0.000\n##    .y1                0.000                               0.000    0.000\n##    .y2                0.000                               0.000    0.000\n##    .y3                0.000                               0.000    0.000\n\n\nPlotting model fit\nNow, we can plot the model-fitted values. To examine how the model relates to the data, we’ll follow the principle of the model which is to partition the within versus between subject variance. You can reinforce the corresponding intuition by looking back at the path diagram: keep in mind that every observed value will be exactly equal to the wave mean, the individual’s latent intercept, and the per-wave latent residual (that is, p and q). So first, we’ll plot the individual variation around the wave-wise means (the stable, between subject individual differences captured by \\(\\kappa\\) and \\(\\omega\\)), along with the observed values. You can see that there is a lot of distance between the lines (that is, the expected values based on the random intercept and wave-wise means) and the observed values. It is that deviation that the within-subject portion of the model (the cross-lagged part) is attempting to explain.\n#get the model-expected means\nmeans &lt;- fitted(fitConstrainedARCL)$mean\nmeansDF &lt;- data.frame(mean = means, key = names(means)) %&gt;%\n  extract(col = key, into = c('var', 'wave'), regex = '(\\\\w)(\\\\d)')\n\n#plot the model-expected random intercepts\npredict(fitConstrainedARCL) %&gt;%\n  as.data.frame %&gt;%\n  mutate(pid = 1:n()) %&gt;%\n  gather(key, latentvalue, -pid, -kappa, -omega) %&gt;%\n  extract(col = key, into = c('latentvar', 'wave'), regex = '(\\\\w)(\\\\d)') %&gt;%\n  mutate(var = c(p = 'x', q = 'y')[latentvar]) %&gt;%\n  left_join(meansDF) %&gt;% #those means from above\n  left_join(antireadLong, by = c('pid', 'wave', 'var')) %&gt;% #the raw data\n  mutate(expectedLine = ifelse(var == 'x', kappa, omega) + mean,\n         wave = as.numeric(wave)) %&gt;%\n  rowwise() %&gt;%\n  ggplot(aes(x = wave, y = expectedLine, color = var, group = var)) +\n  geom_point(aes(x = wave, y = value, group = interaction(var, pid)), alpha = .1, position = position_jitter(w = .2, h = 0)) +\n  geom_line(aes(y = expectedLine, group = interaction(var, pid)), stat = 'identity', alpha = .1) + \n  geom_line(aes(y = mean), stat = 'identity', alpha = 1, size = 1, color = 'black') + \n  facet_wrap(~var, ncol = 2) + \n  theme_classic()\n\nWe can also look at the correlations between the latent residuals, which are essentially the parts of the observed scores that are not accounted for by the stable individual differences in the above graph.\n{% quote hamaker_critique_2015 %} That is, the autoregressive parameters \\(\\alpha^{*}_{t}\\) and \\(\\delta^{*}_{t}\\) do not represent the stability of the rank order of individuals from one occasion to the next, but rather the amount of within-person carry-over effect (cf., Hamaker, 2012; Kuppens, Allen, & Sheeber, 2010; Suls, Green, & Hillis, 1998): If it is positive, it implies that occasions on which a person scored above his or her expected score are likely to be followed by occasions on which he or she still scores above the expected score again, and vice versa. (p. 104-105) {% endquote %}\nSo you may interpret the raw correlations in the graph below as the basis for the constrained model estimates of the path weights above. In the interest of checking against over-interpreting these relations, though, here is the authors’ footnote to the above statement:\n{% quote hamaker_critique_2015 %} One could also say these autoregressive parameters indicate the stability of the rank-order of individual deviations, but this is less appealing from a substantive viewpoint. (p. 105) {% endquote %}\nlibrary(GGally)\npredict(fitConstrainedARCL) %&gt;%\n  as.data.frame %&gt;%\n  select(-kappa, -omega) %&gt;%\n  ggpairs(lower = list(continuous = wrap(ggally_smooth, alpha = .5))) + \n  theme_classic()\n\nNote how different, and one might say diminished, the relations in the above graph are versus the relations in the scatter-plot matrix of the raw data, below. The strength of this model seems to lie in its ability to keep one from being fooled into a within-person explanation of what are largely between-person relations.\nantiread %&gt;% \n  select(-x4, -y4) %&gt;%\n  ggpairs(lower = list(continuous = wrap(ggally_smooth, alpha = .5))) + \n  theme_classic()"
  },
  {
    "objectID": "thankyou.html",
    "href": "thankyou.html",
    "title": "Thank You",
    "section": "",
    "text": "Thank you! I will email you shortly."
  },
  {
    "objectID": "projects.html",
    "href": "projects.html",
    "title": "Projects & Experiments",
    "section": "",
    "text": "These projects reflect continuous skill development across statistical modeling, computational tools, data visualization, and AI/ML applications. Each one represents an opportunity to learn something new—whether that’s a statistical technique, a software engineering practice, or a creative application of computational thinking."
  },
  {
    "objectID": "projects.html#statistical-modeling-analysis",
    "href": "projects.html#statistical-modeling-analysis",
    "title": "Projects & Experiments",
    "section": "Statistical Modeling & Analysis",
    "text": "Statistical Modeling & Analysis\n\nPhD Unemployment Model\nBayesian time series analysis of PhD unemployment rates relative to general unemployment and other graduate degree holders, using Current Population Survey microdata from IPUMS USA.\nWhat I’m learning: Bayesian time series decomposition (GAMs, Gaussian processes, autoregressive structures), handling massive survey datasets (40M+ rows with data.table and fst for fast I/O), and applying test-driven development to statistical modeling workflows. The project uses Stan via brms and cmdstanr for principled uncertainty quantification across multiple seasonal components, trends, and economic cycles.\nSkills: R, Stan/brms, data.table, Bayesian time series, GAMs, TDD for statistical code\n\n\nGarmin Health Data Analysis\nBayesian analysis of personal wearable health data from Garmin, with interactive D3.js visualizations for exploring trends and cross-metric relationships.\nWhat I’m learning: Applying Gaussian process models to noisy personal health time series (weight, sleep, activity, heart rate), building interactive web visualizations with D3.js, and working with Python’s scientific stack (uv, CmdStan). The project bridges statistical rigor with practical self-quantification.\nSkills: Python, Stan, Gaussian processes, D3.js, interactive data visualization, CmdStan\n\n\nfMRI Puberty Analysis\nNeuroscience analysis examining the relationship between pubertal development measures and neural activity during a reward/loss task, using the Cole-Anticevic Brain-wide Network Partition for ROI definition.\nWhat I’m learning: Applying TDD methodology to neuroimaging analysis pipelines—writing tests for data validation, model convergence, and statistical properties before implementing analyses. The project uses nested model comparison (sex, age, pubertal timing, hormones) with AIC-based selection and partial R2 estimation.\nSkills: R, neuroimaging analysis, nested model comparison, TDD for research code, psychometrics\n\n\nMeasurement Invariance Mapping\nExploratory work on measurement invariance testing and visualization approaches for psychometric data.\nSkills: Python, Jupyter, psychometrics, measurement invariance"
  },
  {
    "objectID": "projects.html#aiml-generative-systems",
    "href": "projects.html#aiml-generative-systems",
    "title": "Projects & Experiments",
    "section": "AI/ML & Generative Systems",
    "text": "AI/ML & Generative Systems\n\nImage Generation Pipeline v2\nA Node.js rewrite of my earlier Python SDXL pipeline, implementing beam search—a standard ML algorithm for exploring multiple promising paths simultaneously—applied to iterative prompt refinement for AI image generation.\nWhat I’m learning: How ML search algorithms translate when working with LLM and image generation APIs rather than traditional model architectures. The pipeline integrates GPT-4 (prompt generation), DALL-E 3 (image creation), and GPT-4V (evaluation) with dual scoring on alignment and aesthetics. Built with 410+ tests and a React frontend with real-time WebSocket progress updates.\nSkills: Node.js, beam search, multi-model API integration, React, WebSockets, TDD (410+ tests)\n\n\nSDXL Prompt Generation & Evaluation\nThe original Python pipeline for automated prompt refinement during Stable Diffusion XL image generation, using CLIP scoring for semantic alignment, aesthetic prediction models, and vision-language captioning.\nWhat I’m learning: Hands-on experience with diffusion model pipelines, LoRA adapter integration, CLIP embeddings for image-text alignment, and building multi-stage ML pipelines that chain generation, evaluation, and refinement.\nSkills: Python, PyTorch, SDXL/diffusers, CLIP, LoRA, aesthetic scoring, LLM prompt engineering\n\n\nStory Time\nA web-based tool for writers to expand, refine, revise, and restructure narratives using locally-hosted language models—keeping creative control and privacy through local LLM execution.\nWhat I’m learning: Designing flexible text operation interfaces (expand, refine, revise, restructure), managing narrative context for LLMs (user-provided synopsis vs. auto-generated summaries), and building tools that augment rather than replace human creativity.\nSkills: JavaScript, local LLM integration (Ollama/vLLM/HuggingFace), web development, UX design for AI tools\n\n\nUnderstanding LLMs\nA hands-on exploration of how language models actually work—not just how to use them, but how they represent and process information internally.\nWhat I’m learning: Transformer architecture internals (attention, residual streams, MLPs), mechanistic interpretability techniques (probing, sparse autoencoders, activation patching, steering vectors), and what actually changes during fine-tuning (LoRA, adapters). All experiments run locally on a 12GB GPU using smaller models (GPT-2, Pythia).\nSkills: Python, PyTorch, transformer internals, mechanistic interpretability, local GPU computing\n\n\nImage Organizer\nAI-powered image organization system for large local photo collections (100k+ images), with automatic tagging via multiple ML models, RAW file support, and semantic search.\nWhat I’m learning: Building ML-powered desktop applications with PyTorch and Transformers for vision models, designing non-destructive metadata workflows with sidecar files, and implementing embedding-based semantic search with ChromaDB/pgvector.\nSkills: Python, PyTorch, Transformers, FastAPI, RAW image processing, embedding search, TDD"
  },
  {
    "objectID": "projects.html#data-visualization-computational-tools",
    "href": "projects.html#data-visualization-computational-tools",
    "title": "Projects & Experiments",
    "section": "Data Visualization & Computational Tools",
    "text": "Data Visualization & Computational Tools\n\nTerrain Maker v2\nPrimary Language: Python GeoTIFF terrain visualization and analysis with Blender rendering, integrated with Claude Code TDD workflows. Creates stunning 3D terrain visualizations from elevation data.\n\n\nWealth Stratification Agent-Based Model\nPrimary Language: JavaScript Agent-based modeling of wealth stratification dynamics, exploring emergent patterns in economic inequality through computational simulation.\n\n\nHamilton Street Architectural Design\nPrimary Language: JavaScript TDD-driven architectural design system for multi-family residential development at 4081 Hamilton Street. Features graph-based topology validation for building layout optimization.\n\n\nLEGO Builder\nPrimary Language: HTML Interactive LEGO construction tool or visualization system, demonstrating computational geometry and 3D modeling concepts."
  },
  {
    "objectID": "projects.html#research-publications-infrastructure",
    "href": "projects.html#research-publications-infrastructure",
    "title": "Projects & Experiments",
    "section": "Research Publications & Infrastructure",
    "text": "Research Publications & Infrastructure\n\nNo Silver Bullets: Software Cycle Time\nRepository for the peer-reviewed paper “No Silver Bullets: Why Understanding Software Cycle Time is Messy, Not Magic” (arXiv:2503.05040). Analyzes 55,000+ observations across 216 organizations using Bayesian hierarchical modeling to separate individual and organizational variation in software development velocity.\nKey finding: Common workplace factors have precise but modest effects on cycle time, set against considerable unexplained variation—suggesting systems-level thinking rather than individual-focused interventions.\nSkills: Bayesian hierarchical modeling, Stan/brms, large-scale observational data analysis, scientific writing\n\n\nDocker: verse-cmdstan\nDocker container extending rocker/verse with pre-compiled CmdStan for reproducible Bayesian computing environments.\nSkills: Docker, reproducible computing infrastructure, Stan ecosystem\n\n\nClaude Code Setup\nA working reference implementation of professional Claude Code development workflows, installable via npx. Provides battle-tested command templates for TDD, documentation analysis, project health monitoring, and AI-assisted development.\nWhat I’m learning: Building and distributing NPM packages, designing developer tooling that encodes best practices, and integrating AI coding assistants into rigorous development workflows.\nSkills: JavaScript, NPM package development, CLI tooling, TDD workflow design"
  },
  {
    "objectID": "projects.html#web-development",
    "href": "projects.html#web-development",
    "title": "Projects & Experiments",
    "section": "Web Development",
    "text": "Web Development\n\nPersonal Website\nSource for this site—built with Quarto, deployed via GitHub Pages.\n\n\nmelanieberry.com\nWebsite development for a client/collaborator.\n\n\nTrue Life Center Proposal\nTechnical proposal and project planning materials.\n\n\n\n\n\n\n\nCross-Cutting Themes\n\n\n\nAcross these projects, several themes emerge:\n\nTest-driven development everywhere: From statistical models to architectural designs to ML pipelines—TDD isn’t just for web apps. Writing tests first clarifies requirements, validates assumptions, and enables safe iteration.\nBayesian thinking as a unifying framework: Whether modeling PhD unemployment, health trends, or software cycle time, Bayesian methods provide principled uncertainty quantification and hierarchical structure for complex data.\nBridging domains: Applying research methodology skills (measurement theory, causal inference, study design) to new domains like software engineering, architecture, and creative AI tools.\nLearning by building: Each project is a vehicle for understanding something deeply—beam search, transformer internals, geospatial processing, agent-based dynamics—by implementing it from scratch.\nAI-augmented development: Many projects use Claude Code with TDD workflows, exploring how AI coding assistants can enhance (not replace) rigorous software development practices."
  },
  {
    "objectID": "projects.html#simulation-computational-modeling",
    "href": "projects.html#simulation-computational-modeling",
    "title": "Projects & Experiments",
    "section": "Simulation & Computational Modeling",
    "text": "Simulation & Computational Modeling\n\nWealth Stratification Agent-Based Model\nAn agent-based model demonstrating how wealth inequality emerges across generations through genetic potential, environmental factors, assortative mating, and stochastic events. Features a live interactive demo.\nWhat I’m learning: Agent-based modeling as a tool for understanding emergent social phenomena, implementing intergenerational dynamics (allele inheritance, environmental endowment, wealth transmission), and building interactive browser-based simulations. The model uses log-normal + Pareto wealth distributions and tracks Gini coefficients across generations.\nSkills: JavaScript, agent-based modeling, wealth distributions, interactive simulation, D3.js\n\n\nHamilton Street Architectural Design\nA fully validated architectural design system for a 2-story duplex, using genetic algorithm optimization and graph-based topology validation. All 216 tests prove compliance with San Diego zoning, building codes, and functional constraints.\nWhat I’m learning: Applying TDD and constraint satisfaction to a completely different domain—architecture. Traditional design iterates through draw-check-fix cycles over weeks; this approach defines constraints first, then generates designs that are mathematically guaranteed to comply. Features SVG floor plan generation and genetic algorithm optimization for layout.\nSkills: JavaScript, genetic algorithms, constraint satisfaction, graph-based validation, SVG generation, TDD (216 tests)"
  },
  {
    "objectID": "projects.html#data-visualization-geospatial-tools",
    "href": "projects.html#data-visualization-geospatial-tools",
    "title": "Projects & Experiments",
    "section": "Data Visualization & Geospatial Tools",
    "text": "Data Visualization & Geospatial Tools\n\nTerrain Maker v2\nA Python library for creating custom terrain visualizations from Digital Elevation Models with Blender 3D rendering, supporting arbitrary spatial data overlays and multi-view rendering from any cardinal direction.\nWhat I’m learning: Geospatial data processing (SRTM tiles, GeoTIFF, reprojection, resampling), programmatic 3D rendering with Blender’s Python API, and building reusable scientific visualization libraries with smart caching. The Detroit example generates 1.3M-vertex terrain meshes with color-mapped data overlays.\nSkills: Python, Blender API, GeoTIFF/DEM processing, geospatial operations, 3D visualization, hash-based caching\n\n\nLEGO Builder\nInteractive LEGO construction and visualization tool.\nSkills: HTML/JavaScript, 3D visualization, computational geometry"
  }
]